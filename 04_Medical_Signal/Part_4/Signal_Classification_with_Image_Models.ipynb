{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShuYuHuang/CMU_Course_signal/blob/main/04_Medical_Signal/Part_4/Signal_Classification_with_Image_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Signal Segmentation with Image Model 主題簡介"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "雖然ECG或其他訊號通常是一維訊號（先排除通道間的相關性），但這並不意味著它們無法被二維的影像模型處理。事實上，由於網路上關於影像的預訓練模型較多，我們可以利用這些模型的強大能力來分析和預測醫學訊號。透過將訊號轉換為影像，我們可以更好地利用這些預訓練模型。\n",
        "\n",
        "不同影像模型可以完成不同AI訊號處理、分析，可能包含signal classification, signal segmentation, signal generation等等等。我們這邊會以signal classification 為例，讓各位學習如何以處理圖形用的模型來處理影像分類。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎡教學目標"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 時頻分析工具使用: 使用torchaudio、librosa等工具將訊號進行時頻分析，成為時頻圖。\n",
        "- 模型建構: 使用Hugginface中預訓練模型做訓練。\n",
        "- 資料載入: 使用torchaudio及Torch之Datasets Class準備載入資料，包含讀取、前處理等等。\n",
        "- 模型訓練: 使用hugginface transformer 的Trainer 模組訓練模型並調整超參數以達到最佳效果。\n",
        "- 模型評估: 使用開源套件計算評估指標，利用測試資料集評估模型效能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAgWEtENTe_0"
      },
      "source": [
        "# 📈時頻分析工具使用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVFZyj0Tkz8"
      },
      "source": [
        "這邊我們將介紹使用python一些訊號工具，對訊號做時頻分析，得到2D的時頻圖，這樣就能夠有CNN所倚仗的local connectivity性質，適合使用2D CNN。我們這邊應用在ECG訊號的處裡上，不過其中很多概念與工具都可以用在其他訊號的處理。\n",
        "\n",
        "課程包含以下內容:\n",
        "* Up/Down Sampling\n",
        "* Fast Fourier Transform (FFT)\n",
        "* Short-Time Fourier Transform (STFT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBuA4UZbUqkD"
      },
      "source": [
        "# 前置準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygnYUkkVmtYg"
      },
      "outputs": [],
      "source": [
        "# For server or other condition that you might have to reinstall numba for librosa\n",
        "# %pip uninstall -y numba\n",
        "# %pip uninstall -y llvmlite\n",
        "# %conda install -y -c numba/label/dev llvmlite\n",
        "# %pip install git+https://github.com/numba/numba\n",
        "\n",
        "# For Colab, just install librosa\n",
        "%pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEbeDIO3mtYi"
      },
      "outputs": [],
      "source": [
        "# For everyone\n",
        "%pip -q install transformers==4.30.0 datasets evaluate accelerate torchaudio torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3EXnuslEqz3"
      },
      "outputs": [],
      "source": [
        "# 取得ECG單檔\n",
        "!wget https://github.com/ShuYuHuang/ai4ecg/releases/download/example_data/ecg_example.csv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj4KfMyJEwQH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd  # 導入pandas套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VijkLelD2k-k"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"ecg_example.csv\")  # 讀取CSV文件\n",
        "sr = 500\n",
        "signal = data.values[:, 1:]  # 從數據中提取訊號\n",
        "signal.shape  # 顯示signal的形狀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEITsHC35Xpz"
      },
      "source": [
        "### Torchaudio.Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR30ENqg5iRD"
      },
      "source": [
        "我們使用torchaudio做為torch原生處理signal 的方法集\n",
        "- 可以針對toch tensor作用\n",
        "- 視tensor device位置，可以在 CPU 及 GPU 作用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlDr7pPJ39r6"
      },
      "outputs": [],
      "source": [
        "# 這邊為了後面使用torchaudio的functions，我們將input轉為tensor使用\n",
        "# torchaudio.transforms 作用的資料shape: (BATCH,) ...,TIME\n",
        "# 總之最後一個axis必須要是時間，前面如何排列都無所謂，因此，我們從csv檔讀取的資料須經過transpose\n",
        "x = torch.tensor(signal, dtype=torch.float32).transpose(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDu7v9Wa5i5x"
      },
      "outputs": [],
      "source": [
        "import torchaudio.transforms as T  # 載入處裡方法集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABFrHj0_4Aqz"
      },
      "source": [
        "## Re-Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMgCqPAl4h6D"
      },
      "source": [
        "Sampling rate的調整是處理訊號重要的一環，要是要將兩個訊號疊加，但發現兩者雖然sample數相同sampling rage卻不相符則會有錯誤的疊加效果。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYJK_Z2U4j_D"
      },
      "source": [
        "Re- sampling有各種方法，對訊號皆有擾動，詳細可參考torch官網參數```resampling_method```:\n",
        "https://pytorch.org/audio/stable/generated/torchaudio.transforms.Resample.html?highlight=resample#torchaudio.transforms.Resample\n",
        "\n",
        "預設的就一般訊號分析來講很夠用了。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-4g_VUM4CLK"
      },
      "source": [
        "### Downsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR2CDmVv5KOT"
      },
      "outputs": [],
      "source": [
        "# Downsample 減少sampling rate,也減少資料點數\n",
        "downsample = T.Resample(orig_freq = sr, new_freq = sr//4)\n",
        "x_125 = downsample(x)\n",
        "print(x.shape, x_125.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDQteH_a6m5L"
      },
      "outputs": [],
      "source": [
        "# 同樣是10秒錄音，在resample後點數會變少\n",
        "print(len(x.T), len(x_125.T))\n",
        "# 若是需求的sampling rate剛好是原本的因數，那也可以直接依downsample的倍率做sample\n",
        "x_125 = x[:, ::4]\n",
        "print(len(x.T), len(x_125.T))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9qKkFcQ9AYT"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display as ldp\n",
        "\n",
        "# 從waveplot上可以看到在經過downsample後 有點失真了\n",
        "ldp.waveshow(x[0, :int(0.5*sr)].numpy(), sr=sr)\n",
        "ldp.waveshow(x_125[0, :int(0.5*sr//4)].numpy(), sr=sr//4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiMYoKDu4HKL"
      },
      "source": [
        "### Upsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFcbCHhh7TUz"
      },
      "outputs": [],
      "source": [
        "# Upsample 增加sampling rate,也增加資料點數\n",
        "upsample = T.Resample(orig_freq = sr, new_freq = sr*2)\n",
        "x_1000 = upsample(x)\n",
        "print(x.shape, x_1000.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1M5xN_L77D7"
      },
      "outputs": [],
      "source": [
        "# 在upsample時不會有失真的情況，因為是類似在資料間插值補值\n",
        "ldp.waveshow(x[0, :int(0.5*sr)].numpy(), sr=sr)\n",
        "ldp.waveshow(x_1000[0, :int(0.5*sr*2)].numpy(), sr=sr*2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z5K7hcg4HQK"
      },
      "source": [
        "一般來講upsampling都不存在標準答案，因為就是sampling rate不夠才要補。\n",
        "\n",
        "Upsampling本身後來在AI領域也變成一個議題，叫Super Resolution，使用訓練好的神經網路模型來做upsampling增加解析度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_POOkrU4HS1"
      },
      "source": [
        "## Fast Fourier Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRqwqVEI4HVj"
      },
      "source": [
        "分析訊號的成分最常見的方式是頻譜分析，連續訊號可以藉由Fourier Transform得到頻譜，也就是訊號在頻率上的分布。\n",
        "\n",
        "而對於數位訊號，我們可以藉由Discrete Fourier Transform得到頻譜\n",
        "$𝐗_k≔\\sum\\limits_{𝑛=0}^{𝑁−1}𝑥_𝑛 𝑒^{(−𝑗2𝜋𝑘𝑛/𝑁)}$\n",
        "\n",
        "<img src=https://hackmd.io/_uploads/SyMGrDWY3.png width=800>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXHGt0AB4WaT"
      },
      "source": [
        "Scipy或者其他套件包有提供一些方式做快速的Discrete Fourier Transform，稱為Fast Fourier Transform(FFT)。\n",
        "\n",
        "若是訊號分析則用Scipy就好，若是要整合到神經網路上可能就得使用Tensorflow或Pytorch內建的fft。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t05NZuGX4T2q"
      },
      "outputs": [],
      "source": [
        "from scipy import fft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2DGvXyE-CPs"
      },
      "source": [
        "這邊因為我們做的是離散的Fourier轉換(時間點數、頻率點數)，若要看到原本單位(seconds, Hz)則需進行轉換。\n",
        "* 時間: $t=n/f_s$ =>時間刻度 $\\Delta t=1/f_s$\n",
        "* 頻率: $f=k f_s/N$ =>頻率刻度 $\\Delta f=f_s/N$\n",
        "\n",
        "N為參與FFT的時間點數量，$f_s$則是sampling rate。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn-CnhUZ_xnt"
      },
      "source": [
        "**e.g. 假設我們有一段訊號，其採樣率為 200Hz，我們使用 500 點 FFT 進行頻譜分析，我們想要取得 0.2Hz~1.2Hz 這個頻段的頻率響應(頻譜數值)如何選取index?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDm2XzUVEPQh"
      },
      "source": [
        "首先，我們需要計算頻譜的採樣間隔。\n",
        "\n",
        "頻率刻度計算考慮sampling rate $f_s = 200Hz，FFT點數N = 500$，因此 $\\Delta f = 200/500 = 0.4Hz$。\n",
        "\n",
        "其次，我們需要計算 0.2Hz~1.2Hz 這個頻段的index k 值且$k = f / \\Delta f $\n",
        "，因此 計算最低與最高k值為：\n",
        "\n",
        "- 最低 = 0.2Hz / 0.4Hz = 0.5\n",
        "- 最高 = 1.2Hz / 0.4Hz = 3\n",
        "\n",
        "因此，我們需要篩出頻譜中 k 值為1\\~3的頻譜，得到[0.4, 0.8, 1.2] Hz的頻率響應。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TknSFwfG-SRU"
      },
      "outputs": [],
      "source": [
        "signal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PqIM_01-ErM"
      },
      "outputs": [],
      "source": [
        "# 使用scipy.fft.fft可以對訊號做fft\n",
        "#  'x' - 資料\n",
        "#  'n' - FFT點數，通常不指定，預設為資料總點數\n",
        "# 記得做完Fourier Transform後m, 出來都是複數 (預設為complex64格式)\n",
        "# (不管是continuous/discrete/fast Fourier Transfor)\n",
        "N = len(signal)\n",
        "x_f = fft.fft(signal[:N,0])  # TIME => FREQUENCY (only FREQUENCY/2 range is valid)\n",
        "x_f.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E40CqfNRDQEO"
      },
      "outputs": [],
      "source": [
        "frequency = np.linspace(0, sr, N)\n",
        "max_f = 10  # 設定想看到的最大頻率 (Hz)\n",
        "max_k = int(max_f*N/sr)  # 轉成k\n",
        "\n",
        "# 通常我們是看這個複數的magnitude, 取absolute就可以做到\n",
        "plt.plot(frequency[:max_k], abs(x_f[:max_k]))\n",
        "plt.xlabel(\"frequency(Hz)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-ySY9Zv-nUD"
      },
      "source": [
        "在頻譜圖中我們可以看到較為高峰的點就表示訊號有較多該頻率成分。\n",
        "\n",
        "我們可以試著把做頻譜的時間縮點一點，來看看最前面一段的頻譜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGTzHZvx-oeM"
      },
      "outputs": [],
      "source": [
        "N1 = int(0*sr)\n",
        "N2 = int(5*sr)\n",
        "plt.plot(np.arange(N1, N2)/sr, signal[N1:N2, 0])\n",
        "plt.xlabel(\"time(s)\")\n",
        "plt.show()\n",
        "x_f2 = fft.fft(signal[N1:N2, 0])\n",
        "frequency2 = np.linspace(0, sr, N2-N1)\n",
        "max_k2 = int(max_f*(N2-N1)/sr)  # 轉成k\n",
        "plt.plot(frequency2[:max_k2], abs(x_f2[:max_k2]))\n",
        "plt.xlabel(\"frequency(Hz)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9VPM-tw_dVk"
      },
      "source": [
        "我們可以看到較前面一小段的頻譜跟一整段的峰出現頻率差不多，因為心跳隨著時間有穩定的節奏性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnyjTAXI_c08"
      },
      "outputs": [],
      "source": [
        "plt.plot(frequency[:max_k], abs(x_f[:max_k]))\n",
        "plt.plot(frequency2[:max_k2], abs(x_f2[:max_k2]))\n",
        "plt.xlabel(\"frequency(Hz)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf2R3m0WAjKc"
      },
      "source": [
        "擷取與整段對比:\n",
        "* 整段時長較長，堆積出的能量比較高"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F-9qQjiA9G0"
      },
      "source": [
        "## Short-Time Fourier Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJXz-x_IBAAs"
      },
      "source": [
        "若是想統計出一段整段的頻率分析，則這個FFT就夠用了，但通常想知道的是在每個時間點頻率分布長什麼樣子就必須要做時頻分析。\n",
        "\n",
        "時頻分析中最常見的方式就是Sort-Time Fourier Transform:\n",
        "1. Windowing\n",
        "2. 個別做頻譜\n",
        "\n",
        "公式:\n",
        "\n",
        "$𝑋[q,k] = \\sum\\limits_{𝑛^′=⌈−𝑁/2⌉}^{⌈𝑁/2⌉−1}𝑥[𝑛^′+𝑞𝐻]𝑤[𝑛^′] 𝑒^\\frac{−𝑗2\\pi𝑘𝑛^′}{𝑁}$\n",
        "\n",
        "* q- 每個window的離散時間點\n",
        "* k- 離散頻率點\n",
        "* n'- 原訊號的離散時間點\n",
        "* N- window內FFT點數，同時是window size\n",
        "* H- hop size，每個window離多少n'\n",
        "* w- windowing function\n",
        "* x- signal\n",
        "\n",
        "<img src=https://hackmd.io/_uploads/S1K7SPZK3.png width = 600>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP_Zaka9BUuc"
      },
      "source": [
        "我們這邊使用```torchaudio.transforms.Spectrogram```來做時頻分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1jiReFK566G"
      },
      "outputs": [],
      "source": [
        "# 使用T.Spectrogram可以形成一個做stft的function\n",
        "#  'n_fft' - FFT點數，同時是window長度，這邊一定要指定，預設400，可根據自己想看到的頻率範圍調整\n",
        "#  'hop_length' - 每個window間要跳多長\n",
        "#  'window_fn' - windowing function，預設是 'torch.hann_window'\n",
        "# 出來是Spectrogram, type由input type指定\n",
        "\n",
        "stft = T.Spectrogram(n_fft=128) # Calculate spectrogram with STFT\n",
        "S = stft(x)  # (BATCH,) CHANNELS, TIME => (BATCH,) CHANNELS, FREQUENCY, TIME\n",
        "S.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiF-a3GAB548"
      },
      "outputs": [],
      "source": [
        "# 可以看出，我們使用STFT可生出一個K x Q 的矩陣\n",
        "print(S.shape)\n",
        "print(type(S))\n",
        "print(S.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNFhNOI4Eoil"
      },
      "source": [
        "使用```librosa.display.specshow```可以畫出頻譜圖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGaAO52FEtbs"
      },
      "outputs": [],
      "source": [
        "# 使用librosa.display.specshow畫出Spectrogram\n",
        "#  'data' - Spectrogram\n",
        "#  'sr' - sampling rate\n",
        "#  'x_axis' - x 軸刻度單位\n",
        "#  'y_axis' - y 軸刻度單位，預設為 'hz'，若要使用log scale可以用 'log'\n",
        "#  'cmap' - color map\n",
        "#  'hop_length' - hop length，要跟著前面\n",
        "#  'n_fft' - FFT點數，要跟著前面\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "ldp.specshow(S[0].numpy(),\n",
        "             n_fft=128,\n",
        "             hop_length=128//2,\n",
        "             sr=sr,\n",
        "             x_axis=\"s\",\n",
        "             y_axis=\"hz\",\n",
        "             cmap=\"gray\")\n",
        "plt.colorbar(format=\"%+4.f\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ0hioDzGCnP"
      },
      "source": [
        "若全圖檢視可以看到所有包含的頻率，但其中Y軸的scale有點太大使得較低頻較有資訊的部分看不見，可以轉而使用log scale frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYpBJd6iGB_t"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 5))\n",
        "ldp.specshow(S[0].numpy(),\n",
        "             n_fft=128,\n",
        "             hop_length=128//2,\n",
        "             sr=sr,\n",
        "             x_axis=\"s\",\n",
        "             y_axis=\"log\",\n",
        "             cmap=\"gray\")  # **\n",
        "plt.colorbar(format=\"%+4.f\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GiDitfkGR2E"
      },
      "source": [
        "結果如果對比不夠明顯，可以使用```librosa.power_to_db```轉成分貝來看"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps0hmmi8GUf9"
      },
      "outputs": [],
      "source": [
        "S_db = librosa.power_to_db(abs(S[0]).numpy()) # **\n",
        "plt.figure(figsize=(6, 5))\n",
        "ldp.specshow(S_db,\n",
        "             n_fft=128,\n",
        "             hop_length=128//2,\n",
        "             sr=sr,\n",
        "             x_axis=\"s\",\n",
        "             y_axis=\"log\",\n",
        "             cmap=\"gray\")  # **\n",
        "plt.colorbar(format=\"%+2.f\")\n",
        "plt.clim([-10, 10])  # **\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28R_q46IuAe"
      },
      "source": [
        "這邊就是曹昱老師講音訊處裡AI時的前置步驟，feature extraction那一塊做的事情\n",
        "\n",
        "<img src=https://hackmd.io/_uploads/BJ_UkJ3K2.png width=300>\n",
        "\n",
        "(source: 曹老師AIA醫療專班課程)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy_A7qS5M9Tt"
      },
      "outputs": [],
      "source": [
        "# 使用 power=None 使得Spectrogram 返回完整complex number頻譜 torch.complex64\n",
        "stft = T.Spectrogram(n_fft=128, power=None) # Calculate spectrogram with STFT\n",
        "S_c = stft(x)  # (BATCH,) CHANNELS, TIME => (BATCH,) CHANNELS, FREQUENCY, TIME\n",
        "S_c.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVNeYyKROumu"
      },
      "source": [
        "可以使用它的 .real 叫出實數部分、用 .imag 叫出虛數部分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvAg5LPlOIxp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 5))\n",
        "ldp.specshow(S_c[0].real.numpy(),\n",
        "             n_fft=128,\n",
        "             hop_length=128//2,\n",
        "             sr=sr,\n",
        "             x_axis=\"s\",\n",
        "             y_axis=\"log\",\n",
        "             cmap=\"gray\")  # **\n",
        "plt.colorbar(format=\"%+2.f\")\n",
        "plt.clim([-10, 10])  # **\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWQVxtfcOaRO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 5))\n",
        "ldp.specshow(S_c[0].imag.numpy(),\n",
        "             n_fft=128,\n",
        "             hop_length=128//2,\n",
        "             sr=sr,\n",
        "             x_axis=\"s\",\n",
        "             y_axis=\"log\",\n",
        "             cmap=\"gray\")  # **\n",
        "plt.colorbar(format=\"%+2.f\")\n",
        "plt.clim([-10, 10])  # **\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rk5jEWxP4Ve"
      },
      "source": [
        "使用 ```abs```可以將原本complex number的 amplitude部分抓出來，就等於```power=1```的作法，稱為amplitude spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9QEPPC6PrCW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 5))\n",
        "ldp.specshow(abs(S_c[0]).numpy(),\n",
        "             n_fft=128,\n",
        "             hop_length=128//2,\n",
        "             sr=sr,\n",
        "             x_axis=\"s\",\n",
        "             y_axis=\"log\",\n",
        "             cmap=\"gray\")  # **\n",
        "plt.colorbar(format=\"%+2.f\")\n",
        "plt.clim([-10, 10])  # **\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ27WOKEPZR2"
      },
      "source": [
        "使用 ```torch.angle```可以將原本complex number的相位 (實數與虛數的夾角)抓出來，得到phase spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ulHmx7FNsO2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 5))\n",
        "ldp.specshow(torch.angle(S_c[0]).numpy(),\n",
        "             n_fft=128,\n",
        "             hop_length=128//2,\n",
        "             sr=sr,\n",
        "             x_axis=\"s\",\n",
        "             y_axis=\"log\",\n",
        "             cmap=\"gray\")  # **\n",
        "plt.colorbar(format=\"%+2.f\")\n",
        "plt.clim([-10, 10])  # **\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJYzEJtNQmWf"
      },
      "source": [
        "訊號處理中比較常看amplitude spectrum或者power spectrum，所以常以這兩種做為CNN的input。但也可嘗試將phase也加入input，增加資訊。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAy_WfVR1W5W"
      },
      "source": [
        "## Augmentation  (還在想要不要加)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB_L_Hqmwy_j"
      },
      "outputs": [],
      "source": [
        "# from torchaudio.sox_effects import apply_effects_tensor\n",
        "# resample = 100\n",
        "# effects = []\n",
        "# if resample:\n",
        "#     effects.extend(\n",
        "#         [\n",
        "#             [\"lowpass\", f\"{resample // 2}\"],\n",
        "#             [\"rate\", f\"{resample}\"],\n",
        "#         ]\n",
        "#     )\n",
        "# x_, sr_ = apply_effects_tensor(x, sample_rate=500, effects=effects)\n",
        "# S = transform(x_).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tCvtYP4xXnL"
      },
      "outputs": [],
      "source": [
        "# plt.plot(np.linspace(0, 10, len(x_.T)), x_.T[:,0])\n",
        "# plt.plot(np.linspace(0, 10, len(x.T)), x.T[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vawOqj9l3V9w"
      },
      "source": [
        "# 🤗模型建立"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3kCgVgm4QtL"
      },
      "source": [
        "使用與醫療影像同樣的2D CNN model 我們可以針對訊號的頻譜建構classification模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHmyidpW3T05"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "# checkpoint = \"google/efficientnet-b6\"\n",
        "checkpoint = \"facebook/convnext-tiny-224\"\n",
        "# checkpoint = \"microsoft/beit-large-patch16-512\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYTya5VO4beq"
      },
      "outputs": [],
      "source": [
        "labels = ['N', 'O', 'A', '~']\n",
        "label2id = {c: c_idx for c_idx, c in enumerate(labels)}\n",
        "id2label = {str(c_idx): c for c_idx, c in enumerate(label2id)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMUk5NF84Oty"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "第一次使用hugginface的 `AutoModelXXXX.from_pretrained` 預設會將model儲存在暫存資料夾`$HOME/.cache/huggingface`中\n",
        "\n",
        "可以使用系統指令`du`來看到底用了多少容量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -h 可以顯示出常見單位 Byte 作為大小，-d 可以指定表列幾層資料夾\n",
        "!du -h $HOME/.cache/huggingface/hub/ -d 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcU0iOrC41_F"
      },
      "source": [
        "可先嘗試將一個channel的spectrogram放入model。\n",
        "\n",
        "注意這邊要對model做repeat，直到有三個channel，對上model的RGB三個channel。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T04avCMt6agC"
      },
      "outputs": [],
      "source": [
        "# 這邊試著把其中一個channel的Spectrogram丟進Model\n",
        "# S 的 3 個 axis: CH (12), FREQUENCY, TIME\n",
        "# S[0] 的 2 個 axis: FREQUENCY, TIME\n",
        "# Model 的 4 個 axis 需求shape為: BATCH, CH (3), WIDTH, HEIGHT\n",
        "# => 前面多一個axis, 在channel那邊需要\n",
        "x = S[0].repeat(1, 3, 1, 1)\n",
        "with torch.no_grad():\n",
        "    logits = model(x).logits\n",
        "print(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCGso4LI7w1G"
      },
      "source": [
        "# 🗃️資料載入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtUT_T0u7I1h"
      },
      "source": [
        "同Part 2，我們使用CinC2017比賽資料集的一部分來做為訓練資料\n",
        "\n",
        "在colab上要先下載資料集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFH56Pxc7yxP"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "!wget https://github.com/TA-aiacademy/CMU_Course/releases/download/signal_data/cinc_data.tar.gz\n",
        "!mkdir data\n",
        "!tar -xf cinc_data.tar.gz -C ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbhTBJO57ayh"
      },
      "source": [
        "下面針對這種spectrogram，我們做一個 Dataset class。\n",
        "\n",
        "其中會有些參數是考量這筆資料裡面的一些已知內容，例如最小長度，sampling rate等等。\n",
        "\n",
        "在這個例子中我們只使用單channel的spectrogram。\n",
        "\n",
        "那我們已知這個資料集中只有一個channel，所以只要取\"lead\"這個channel就好。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b89Y4pNxRCKK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio.transforms as T  # 載入處裡方法集\n",
        "\n",
        "# Data SPECS:\n",
        "#   sampling rate: 300\n",
        "#   mininal length: ~9 seconds\n",
        "#   maximal length: ~60 seconds\n",
        "#   value range: -7360 ~ 7636\n",
        "class SpectogramDataset(Dataset):\n",
        "    def __init__(self,\n",
        "        data_root,\n",
        "        annot_file,\n",
        "        channel=\"lead\",\n",
        "        class_names=[\"N\", \"O\", \"A\", \"~\"],\n",
        "        n_fft=128,\n",
        "        min_len=300*9,\n",
        "        transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_root (str): root directory of the dataset\n",
        "            annot_file (str): filename of the annotation file, \".json\" format\n",
        "            channel (list): target lead name\n",
        "            class_names (list): list of class names for mapping class name to class id\n",
        "            n_fft (int): point of fft for making spectrogram\n",
        "            min_len (int): length of the output signal, truncated from each full signal\n",
        "\n",
        "        \"\"\"\n",
        "        # Read the label file as list of dictionary\n",
        "        with open(annot_file,\"r\") as f:\n",
        "            self.labels=json.load(f)\n",
        "\n",
        "        # List the signal files\n",
        "        self.data_root = data_root\n",
        "        self.signal_paths = [\n",
        "            os.path.join(data_root, elem[\"csv\"]+\".csv\")\n",
        "            for elem in self.labels\n",
        "        ]\n",
        "\n",
        "        self.class_map = {n: i for i, n in enumerate(class_names)}\n",
        "\n",
        "        print(\"dataset class map: \", self.class_map)\n",
        "        # Set the target leads for the dataset\n",
        "        self.channel = channel\n",
        "        # Set the specs of the dataset\n",
        "        self.min_len = min_len\n",
        "        # Specify method of spectral analysis\n",
        "        self.spectral_analysis = T.Spectrogram(n_fft=n_fft)\n",
        "\n",
        "        # Import image processor if there is one\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_min_len(self):\n",
        "        # Calculate minimal length of signal\n",
        "        # in case you don't know it in advance\n",
        "        signal_len = [\n",
        "            len(self.load_file(file))\n",
        "            for file in self.signal_paths\n",
        "        ]\n",
        "        self.min_len = min(signal_len)\n",
        "        return self.min_len\n",
        "\n",
        "    def truncate(self, x):\n",
        "        # Truncate minimal length from full signal\n",
        "        start = np.random.randint(0, len(x)-self.min_len)  # get a start point of sampling\n",
        "        return x[start:start+self.min_len]\n",
        "\n",
        "    def load_file(self, file_name):\n",
        "        # Load singnal form file\n",
        "        return pd.read_csv(file_name)[self.channel].values\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get one singal and label specific to the given index\n",
        "        signal_path = self.signal_paths[idx]  # get data file path\n",
        "        signal = self.load_file(signal_path)  # load signal\n",
        "        signal = self.truncate(signal)  # get an equal length form file\n",
        "        class_name = self.labels[idx][\"choice\"]  # get label\n",
        "        class_id = self.class_map[class_name]  # map the label to number\n",
        "\n",
        "        # Put data, label into torch tensor\n",
        "        signal = torch.tensor(signal, dtype=torch.float32)\n",
        "        class_id = torch.tensor(class_id, dtype=torch.long)\n",
        "\n",
        "        # Get Spectrogram (repeat 3 times for the CNN model)\n",
        "        signal = self.spectral_analysis(signal).repeat(3, 1, 1)\n",
        "        signal = signal / signal.max() * 255\n",
        "\n",
        "        if self.transforms:\n",
        "            signal = self.transforms(signal)\n",
        "\n",
        "        return {'pixel_values': signal, 'label': class_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVHmSttxYty6"
      },
      "outputs": [],
      "source": [
        "# 這邊皆可使用torchvision來幫忙做前處裡\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    Resize,\n",
        ")\n",
        "\n",
        "# 使用該模型當初訓練時的 mean 跟 STD對數值Normalize\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "\n",
        "# 使用該模型當初訓練時的長寬來做resize\n",
        "size = (image_processor.size[\"shortest_edge\"], image_processor.size[\"shortest_edge\"])\n",
        "resize = Resize(size, antialias=True)\n",
        "\n",
        "# 組合成連續function\n",
        "transforms = Compose(\n",
        "    [\n",
        "        resize,\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "train_ds = SpectogramDataset(\n",
        "    \"./data/csv_train\",\n",
        "    annot_file=\"./data/annot_train.json\",\n",
        "    transforms=transforms)\n",
        "test_ds = SpectogramDataset(\n",
        "    \"./data/csv_test\",\n",
        "    annot_file=\"./data/annot_test.json\",\n",
        "    transforms=transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOv1DTMtZLYZ"
      },
      "outputs": [],
      "source": [
        "# 確認dataset讀得出來，以及他的shape\n",
        "item = next(iter(test_ds))\n",
        "item[\"pixel_values\"].shape # [batch, ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqf6qVQ3KHbo"
      },
      "outputs": [],
      "source": [
        "# 畫一個出來試試看是否OK\n",
        "# 這邊因為做了圖形大小的scaling，所以就不以時頻圖去看而是直接當一般影像看待\n",
        "x = item[\"pixel_values\"].numpy().transpose(1,2,0)\n",
        "x = (x-x.min())/(x.max()-x.min())\n",
        "plt.imshow(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🏭模型訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "有模型及資料準備開始訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 訓練參數"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "下面解說幾種常用參數類型前面講過不過我這邊再附一次:\n",
        "\n",
        "1. 訓練參數相關(主要做ablation study會調控的參數):\n",
        "    - per_device_train_batch_size: 每個裝置訓練的Batch size, Hugginface分散式運算每個GPU的batch大小\n",
        "    - learning_rate: 學習率\n",
        "    - weight_decay: L2 normalization 比率，使weight不至於變化太大\n",
        "2. 訓練內容評估:\n",
        "    - num_train_epochs: EPOCH數\n",
        "    - evaluation_strategy: 多久評估一次，有 'no'、'epoch'、'step'可選，若選step必須指定eval_steps\n",
        "3. 模型儲存\n",
        "    - save_strategy: 多久儲存一次，有 'no'、'epoch'、'step'可選，若選step必須指定save_steps\n",
        "    - save_total_limits: 最多存幾個 model ，請設定以避免硬碟空間爆掉\n",
        "    - metric_for_best_model: 決定最佳模型的計算指標，通常是loss或eval_loss，越小越好\n",
        "    - load_best_model_at_end: 是否在訓練結束時載入最佳模型\n",
        "4. 輸出內容設定\n",
        "    - output_dir: 輸出目錄\n",
        "    - overwrite_output_dir: 是否覆蓋已有目錄及內容\n",
        "    - logging_steps: 紀錄訓練內容的步數(每X步記錄一次)\n",
        "5. 其他:\n",
        "    - remove_unused_columns: 是否刪除沒用到的模型forward輸出值(key+value)，若設為 True 常會使一些判斷項無法運作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I8TZ4AsAhwx"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_model_exercise\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainer物件"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "接下來組建模型Trainer object，可以拿來train模型、evaluate、inference。\n",
        "\n",
        "其中需要代入一些object來initiate這個trainer object。\n",
        "- model: 必須要式HF model\n",
        "- args: 剛剛的TrainingArguments object\n",
        "- train_dataset: 訓練集，HF dataset物件\n",
        "- eval_dataset: 驗證集，HF dataset物件\n",
        "- compute_metrics: 評估指標function，可加可不加，若不加則記錄 loss 跟 eval_loss 而已"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "mymetric = evaluate.load('accuracy')\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return mymetric.compute(predictions=predictions.argmax(1), references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmooMS7VBA7o"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjU8hQbfHx5w"
      },
      "outputs": [],
      "source": [
        "# 版本問題會跳出很多不必要訊息，我們可以拿掉\n",
        "from transformers.integrations import MLflowCallback\n",
        "trainer.remove_callback(MLflowCallback)\n",
        "\n",
        "trainer.train() # 開始訓練模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hge3C-FJJqj9"
      },
      "source": [
        "## Reference (好像應該丟些EEG的)\n",
        "* M. Mueller, Fundamentals of Music Processing, Springer 2015\n",
        "* 前述課本附Notebook- https://www.audiolabs-erlangen.de/resources/MIR/FMP/C0/C0.html\n",
        "* Librosa官網- https://github.com/librosa/librosa\n",
        "* Acoustics for Musicians and Artists, by Miller Puckette, UCSD\n",
        "* Youtube 熱門音訊AI課程- https://github.com/musikalkemist/AudioSignalProcessingForML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q6jMy8BmtY5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
