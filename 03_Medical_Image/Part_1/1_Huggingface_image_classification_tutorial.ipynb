{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e9f2849",
      "metadata": {
        "id": "4e9f2849"
      },
      "source": [
        "# Image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0637cc1",
      "metadata": {
        "id": "d0637cc1"
      },
      "source": [
        "影像分類（image classification）是將影像賦予標籤或類別的過程。不同於文本或音頻分類，輸入是影像上的像素值。影像分類有很多應用，例如在自然災害後檢測損壞情況，監測農作物的健康狀況或幫助篩檢醫學影像中的病徵。\n",
        "\n",
        "以下內容會說明：\n",
        "1. 在 Food-101 資料集上微調 ViT 模型，以便將影像中的食物進行分類。\n",
        "2. 使用微調後的模型進行推論。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f17f4e28",
      "metadata": {
        "id": "f17f4e28"
      },
      "source": [
        ">以下所演示的任務得以使用的模型架構是：[BEiT](https://huggingface.co/docs/transformers/model_doc/beit), [BiT](https://huggingface.co/docs/transformers/model_doc/bit), [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext), [ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2), [CvT](https://huggingface.co/docs/transformers/model_doc/cvt), [Data2VecVision](https://huggingface.co/docs/transformers/model_doc/data2vec-vision), [DeiT](https://huggingface.co/docs/transformers/model_doc/deit), [DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat), [EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer), [EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet), [ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt), [LeViT](https://huggingface.co/docs/transformers/model_doc/levit), [MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1), [MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2), [MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit), [NAT](https://huggingface.co/docs/transformers/model_doc/nat), [Perceiver](https://huggingface.co/docs/transformers/model_doc/perceiver), [PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer), [RegNet](https://huggingface.co/docs/transformers/model_doc/regnet), [ResNet](https://huggingface.co/docs/transformers/model_doc/resnet), [SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer), [Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin), [Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2), [VAN](https://huggingface.co/docs/transformers/model_doc/van), [ViT](https://huggingface.co/docs/transformers/model_doc/vit), [ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid), [ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53fd08c1",
      "metadata": {
        "id": "53fd08c1"
      },
      "source": [
        "在開始之前，確認已安裝所有必要的套件："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeec46a8",
      "metadata": {
        "id": "aeec46a8"
      },
      "source": [
        "Before you begin, make sure you have all the necessary libraries installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7b58ff",
      "metadata": {
        "id": "7f7b58ff"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.30.0 datasets evaluate accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64c919c8",
      "metadata": {
        "id": "64c919c8"
      },
      "source": [
        "## Load Food-101 dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b080b65e",
      "metadata": {
        "id": "b080b65e"
      },
      "source": [
        "首先，從 Hugging Face 資料庫中載入 Food-101 資料集的部分資料。這能預先進行多項實驗，確保在完整資料集上進行更多的訓練前，所有步驟都能夠正常運作。\n",
        "\n",
        "在此，為了簡化流程，將資料集縮減為 10 個類別。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload dataset food-10.zip\n",
        "!wget https://github.com/TA-aiacademy/CMU_Course/releases/download/image_classification_data/food-10.zip\n",
        "!unzip -q food-10.zip"
      ],
      "metadata": {
        "id": "QM0_EWxOj_ZS"
      },
      "id": "QM0_EWxOj_ZS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a5c0b9",
      "metadata": {
        "id": "54a5c0b9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "food = load_dataset(\"food-10/\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78db7b1e",
      "metadata": {
        "id": "78db7b1e"
      },
      "outputs": [],
      "source": [
        "food"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00fa618",
      "metadata": {
        "id": "d00fa618"
      },
      "outputs": [],
      "source": [
        "item = next(iter(food))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abbcaff6",
      "metadata": {
        "id": "abbcaff6"
      },
      "outputs": [],
      "source": [
        "print(item['label'])\n",
        "item['image']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff01f447",
      "metadata": {
        "id": "ff01f447"
      },
      "source": [
        "使用 train_test_split 方法將資料集的訓練部分再進一步切分成訓練集和測試集："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee120be",
      "metadata": {
        "id": "aee120be"
      },
      "outputs": [],
      "source": [
        "food = food.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec56380",
      "metadata": {
        "id": "0ec56380"
      },
      "outputs": [],
      "source": [
        "food"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2caeb4",
      "metadata": {
        "id": "bf2caeb4"
      },
      "source": [
        "查看其中一個樣本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1a718a",
      "metadata": {
        "id": "bd1a718a"
      },
      "outputs": [],
      "source": [
        "food[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4ebc8b",
      "metadata": {
        "id": "ba4ebc8b"
      },
      "source": [
        "資料集中每個範例都有兩個欄位：\n",
        "* image: 包含食物的 PIL(pillow 格式) 影像\n",
        "* label: 食物的標籤類別\n",
        "\n",
        "為了讓模型更容易從標籤 id 中讀取名稱，創建一個將標籤名稱對應到整數以及反對應的字典："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "001d6de3",
      "metadata": {
        "id": "001d6de3"
      },
      "outputs": [],
      "source": [
        "labels = food[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1842dd60",
      "metadata": {
        "id": "1842dd60"
      },
      "outputs": [],
      "source": [
        "labels[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415679dc",
      "metadata": {
        "id": "415679dc"
      },
      "source": [
        "現在，按照標籤 id 轉換成名稱："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50605f4a",
      "metadata": {
        "id": "50605f4a"
      },
      "outputs": [],
      "source": [
        "id2label[str(3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f03cf24",
      "metadata": {
        "id": "5f03cf24"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "234ad681",
      "metadata": {
        "id": "234ad681"
      },
      "source": [
        "## Preprocess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c38b34d",
      "metadata": {
        "id": "8c38b34d"
      },
      "source": [
        "接下來的步驟是載入 ViT 模型使用的影像處理器，將影像處理成張量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e0b4d0",
      "metadata": {
        "id": "99e0b4d0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b87636b",
      "metadata": {
        "id": "4b87636b"
      },
      "outputs": [],
      "source": [
        "image_processor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48692e6b",
      "metadata": {
        "id": "48692e6b"
      },
      "source": [
        "---\n",
        "將影像進行轉換，使模型更具一般性以應付過擬合的情況。這裡會使用的 torchvision 中 transforms 的模組，但也能替換成其他適用的影像處理套件。\n",
        "\n",
        "隨機裁減影像的一部份，將其調整影像大小，並使用影像的平均值和標準差進行標準化："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0426e2e7",
      "metadata": {
        "id": "0426e2e7"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "size = (\n",
        "    image_processor.size[\"shortest_edge\"]\n",
        "    if \"shortest_edge\" in image_processor.size\n",
        "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        ")\n",
        "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f34b375d",
      "metadata": {
        "id": "f34b375d"
      },
      "source": [
        "接下來創建一個預處理函數，轉換並回傳影像的像素值作為模型的輸入："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2abdbc22",
      "metadata": {
        "id": "2abdbc22"
      },
      "outputs": [],
      "source": [
        "def transforms(examples):\n",
        "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "    del examples[\"image\"]\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8ef416",
      "metadata": {
        "id": "4a8ef416"
      },
      "source": [
        "要在整個資料集上應用預處理函數，可以使用 Hugging Face 資料集的 [with_transform](https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.Dataset.with_transform) 方法。當載入資料集的一個元素時，轉換會即時套用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b434c71b",
      "metadata": {
        "id": "b434c71b"
      },
      "outputs": [],
      "source": [
        "food = food.with_transform(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a802e3",
      "metadata": {
        "id": "c2a802e3"
      },
      "outputs": [],
      "source": [
        "food"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266cffeb",
      "metadata": {
        "id": "266cffeb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2df1d5",
      "metadata": {
        "id": "7b2df1d5"
      },
      "outputs": [],
      "source": [
        "item = next(iter(food['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc59d9b1",
      "metadata": {
        "id": "fc59d9b1"
      },
      "outputs": [],
      "source": [
        "print(item['label'])\n",
        "print(item['pixel_values'].size())\n",
        "plt.imshow(torch.permute(item['pixel_values'], (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b831f7",
      "metadata": {
        "id": "90b831f7"
      },
      "source": [
        "現在使用 [DefaultDataCollator](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/data_collator#transformers.DefaultDataCollator) 創建一個批次樣本。與 Hugging face 裡 Transformers 的其他資料收集器不同，DefaultDataCollator 不會套用額外的預處理，例如填充（padding）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914e5bac",
      "metadata": {
        "id": "914e5bac"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29be31d9",
      "metadata": {
        "id": "29be31d9"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940b27b4",
      "metadata": {
        "id": "940b27b4"
      },
      "source": [
        "在訓練過程中加入評估指標通常有助於評估模型的表現。可以使用 Hugging Face 的 [Evaluate](https://huggingface.co/docs/evaluate/index) 函式庫快速載入評估方法。在此任務上載入 [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) 指標（請參閱 Hugging Face 的 Evaluate [快速導覽](https://huggingface.co/docs/evaluate/a_quick_tour)，以了解如何載入和計算指標的詳細資訊）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968afd14",
      "metadata": {
        "id": "968afd14"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a614c9b",
      "metadata": {
        "id": "0a614c9b"
      },
      "source": [
        "然後創建一個函數，將預測及標籤使用 [compute](https://huggingface.co/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute) 以計算準確度："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f46e5104",
      "metadata": {
        "id": "f46e5104"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138d6752",
      "metadata": {
        "id": "138d6752"
      },
      "source": [
        "定義完 compute_metrics 函數，在訓練設定時會再次使用到它。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8f12fb",
      "metadata": {
        "id": "db8f12fb"
      },
      "source": [
        ">如果不熟悉使用 [Trainer]((https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer)) 微調模型，請參考此[基本教程](https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer)！"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c05ee023",
      "metadata": {
        "id": "c05ee023"
      },
      "source": [
        "現在已準備好開始訓練模型了！使用 [AutoModelForImageClassification](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoModelForImageClassification) 載入 ViT。指定標籤的數量以及標籤的對應方式："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53110398",
      "metadata": {
        "id": "53110398"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c0662a6",
      "metadata": {
        "id": "8c0662a6"
      },
      "source": [
        "接著的階段，只剩以下三個步驟：\n",
        "\n",
        "1. 在 [TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments) 中定義訓練的超參數。請務必留意資料集中未使用的資訊，設定 remove_unused_columns=False 可以防止被刪除未使用到的資訊！例如 image，這會導致無法獲得 pixel_values。另一個必需設定的參數是 output_dir，指定模型儲存的位置。通過設定 push_to_hub=True 將模型上傳至 Hub（需要登入 Hugging Face 才能上傳模型）。在每個 epoch 結束時，[Trainer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer) 將評估準確性並儲存訓練模型。\n",
        "2. 將訓練參數、模型、資料集、預處理器、資料收集器以及計算評估指標函數傳遞給 [Trainer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer)。\n",
        "3. 呼叫 [train](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer.train) 來微調模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc45af8",
      "metadata": {
        "id": "ccc45af8"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_food_model\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "#     logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14825bd",
      "metadata": {
        "id": "d14825bd"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=food[\"train\"],\n",
        "    eval_dataset=food[\"test\"],\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3188094d",
      "metadata": {
        "id": "3188094d"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16810085",
      "metadata": {
        "id": "16810085"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d84f43",
      "metadata": {
        "id": "45d84f43"
      },
      "source": [
        "現在，微調後的模型以存放在指定路徑，並可使用它來進行推論！\n",
        "\n",
        "載入想要進行推論的影像："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6d0a49",
      "metadata": {
        "id": "9a6d0a49"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"food-10/\", split=\"validation\")\n",
        "image = ds[\"image\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a61510",
      "metadata": {
        "id": "a2a61510"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12c358e9",
      "metadata": {
        "id": "12c358e9"
      },
      "source": [
        "使用微調後的模型進行推論最簡單的方法是在 pipline() 中設定。藉由指定的模型建構一個影像分類的 pipeline，然後將影像傳遞給它："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9277985a",
      "metadata": {
        "id": "9277985a"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model/checkpoint-141/\")\n",
        "classifier(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb61b52",
      "metadata": {
        "id": "2fb61b52"
      },
      "source": [
        "載入影像處理器對影像進行預處理，並以 PyTorch 的張量型態回傳作為輸入："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7ae3b4",
      "metadata": {
        "id": "ae7ae3b4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "import torch\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"my_awesome_food_model/checkpoint-141/\")\n",
        "inputs = image_processor(image, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a49c9986",
      "metadata": {
        "id": "a49c9986"
      },
      "source": [
        "將輸入傳遞給模型，並回傳 logits（尚未經過 softmax）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7c8784",
      "metadata": {
        "id": "5e7c8784"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\"my_awesome_food_model/checkpoint-141/\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e92350",
      "metadata": {
        "id": "71e92350"
      },
      "outputs": [],
      "source": [
        "predicted_label = logits.argmax(-1).item()\n",
        "model.config.id2label[predicted_label]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df27d90b",
      "metadata": {
        "id": "df27d90b"
      },
      "source": [
        "---\n",
        "## Conclusion - HuggingFace Workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa35714",
      "metadata": {
        "id": "2fa35714"
      },
      "source": [
        "![](https://hackmd.io/_uploads/HkEzgZdwh.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf60dfbe",
      "metadata": {
        "id": "cf60dfbe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}