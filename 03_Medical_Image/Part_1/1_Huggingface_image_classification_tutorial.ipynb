{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9f2849",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0637cc1",
   "metadata": {},
   "source": [
    "影像分類（image classification）是將影像賦予標籤或類別的過程。不同於文本或音頻分類，輸入是影像上的像素值。影像分類有很多應用，例如在自然災害後檢測損壞情況，監測農作物的健康狀況或幫助篩檢醫學影像中的病徵。\n",
    "\n",
    "以下內容會說明：\n",
    "1. 在 Food-101 資料集上微調 ViT 模型，以便將影像中的食物進行分類。\n",
    "2. 使用微調後的模型進行推論。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f4e28",
   "metadata": {},
   "source": [
    ">以下所演示的任務得以使用的模型架構是：[BEiT](https://huggingface.co/docs/transformers/model_doc/beit), [BiT](https://huggingface.co/docs/transformers/model_doc/bit), [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext), [ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2), [CvT](https://huggingface.co/docs/transformers/model_doc/cvt), [Data2VecVision](https://huggingface.co/docs/transformers/model_doc/data2vec-vision), [DeiT](https://huggingface.co/docs/transformers/model_doc/deit), [DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat), [EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer), [EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet), [ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt), [LeViT](https://huggingface.co/docs/transformers/model_doc/levit), [MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1), [MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2), [MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit), [NAT](https://huggingface.co/docs/transformers/model_doc/nat), [Perceiver](https://huggingface.co/docs/transformers/model_doc/perceiver), [PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer), [RegNet](https://huggingface.co/docs/transformers/model_doc/regnet), [ResNet](https://huggingface.co/docs/transformers/model_doc/resnet), [SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer), [Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin), [Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2), [VAN](https://huggingface.co/docs/transformers/model_doc/van), [ViT](https://huggingface.co/docs/transformers/model_doc/vit), [ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid), [ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd08c1",
   "metadata": {},
   "source": [
    "在開始之前，確認已安裝所有必要的套件："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec46a8",
   "metadata": {},
   "source": [
    "Before you begin, make sure you have all the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.30.0 datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c919c8",
   "metadata": {},
   "source": [
    "## Load Food-101 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080b65e",
   "metadata": {},
   "source": [
    "首先，從 Hugging Face 資料庫中載入 Food-101 資料集的部分資料。這能預先進行多項實驗，確保在完整資料集上進行更多的訓練前，所有步驟都能夠正常運作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset(\"/home/jovyan/ta-shared-ii/datas/new_food-10/\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fa618",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(food))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcaff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item['label'])\n",
    "item['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff01f447",
   "metadata": {},
   "source": [
    "使用 train_test_split 方法將資料集的訓練部分再進一步切分成訓練集和測試集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee120be",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec56380",
   "metadata": {},
   "outputs": [],
   "source": [
    "food"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2caeb4",
   "metadata": {},
   "source": [
    "查看其中一個樣本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ebc8b",
   "metadata": {},
   "source": [
    "資料集中每個範例都有兩個欄位：\n",
    "* image: 包含食物的 PIL(pillow 格式) 影像\n",
    "* label: 食物的標籤類別\n",
    "\n",
    "為了讓模型更容易從標籤 id 中讀取名稱，創建一個將標籤名稱對應到整數以及反對應的字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415679dc",
   "metadata": {},
   "source": [
    "現在，按照標籤 id 轉換成名稱："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50605f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03cf24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ad681",
   "metadata": {},
   "source": [
    "## Preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38b34d",
   "metadata": {},
   "source": [
    "接下來的步驟是載入 ViT 模型使用的影像處理器，將影像處理成張量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48692e6b",
   "metadata": {},
   "source": [
    "---\n",
    "將影像進行轉換，使模型更具一般性以應付過擬合的情況。這裡會使用的 torchvision 中 transforms 的模組，但也能替換成其他適用的影像處理套件。\n",
    "\n",
    "隨機裁減影像的一部份，將其調整影像大小，並使用影像的平均值和標準差進行標準化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b375d",
   "metadata": {},
   "source": [
    "接下來創建一個預處理函數，轉換並回傳影像的像素值作為模型的輸入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdbc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ef416",
   "metadata": {},
   "source": [
    "要在整個資料集上應用預處理函數，可以使用 Hugging Face 資料集的 [with_transform](https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.Dataset.with_transform) 方法。當載入資料集的一個元素時，轉換會即時套用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a802e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266cffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2df1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(food['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item['label'])\n",
    "print(item['pixel_values'].size())\n",
    "plt.imshow(torch.permute(item['pixel_values'], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b831f7",
   "metadata": {},
   "source": [
    "現在使用 [DefaultDataCollator](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/data_collator#transformers.DefaultDataCollator) 創建一個批次樣本。與 Hugging face 裡 Transformers 的其他資料收集器不同，DefaultDataCollator 不會套用額外的預處理，例如填充（padding）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be31d9",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b27b4",
   "metadata": {},
   "source": [
    "在訓練過程中加入評估指標通常有助於評估模型的表現。可以使用 Hugging Face 的 [Evaluate](https://huggingface.co/docs/evaluate/index) 函式庫快速載入評估方法。在此任務上載入 [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) 指標（請參閱 Hugging Face 的 Evaluate [快速導覽](https://huggingface.co/docs/evaluate/a_quick_tour)，以了解如何載入和計算指標的詳細資訊）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968afd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a614c9b",
   "metadata": {},
   "source": [
    "然後創建一個函數，將預測及標籤使用 [compute](https://huggingface.co/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute) 以計算準確度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d6752",
   "metadata": {},
   "source": [
    "定義完 compute_metrics 函數，在訓練設定時會再次使用到它。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f12fb",
   "metadata": {},
   "source": [
    ">如果不熟悉使用 [Trainer]((https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer)) 微調模型，請參考此[基本教程](https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer)！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ee023",
   "metadata": {},
   "source": [
    "現在已準備好開始訓練模型了！使用 [AutoModelForImageClassification](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoModelForImageClassification) 載入 ViT。指定標籤的數量以及標籤的對應方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53110398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0662a6",
   "metadata": {},
   "source": [
    "接著的階段，只剩以下三個步驟：\n",
    "\n",
    "1. 在 [TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments) 中定義訓練的超參數。請務必留意資料集中未使用的資訊，設定 remove_unused_columns=False 可以防止被刪除未使用到的資訊！例如 image，這會導致無法獲得 pixel_values。另一個必需設定的參數是 output_dir，指定模型儲存的位置。通過設定 push_to_hub=True 將模型上傳至 Hub（需要登入 Hugging Face 才能上傳模型）。在每個 epoch 結束時，[Trainer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer) 將評估準確性並儲存訓練模型。\n",
    "2. 將訓練參數、模型、資料集、預處理器、資料收集器以及計算評估指標函數傳遞給 [Trainer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer)。\n",
    "3. 呼叫 [train](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer.train) 來微調模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc45af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_food_model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "#     logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14825bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import MLflowCallback\n",
    "trainer.remove_callback(MLflowCallback)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16810085",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d84f43",
   "metadata": {},
   "source": [
    "現在，微調後的模型以存放在指定路徑，並可使用它來進行推論！\n",
    "\n",
    "載入想要進行推論的影像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"/home/jovyan/ta-shared-ii/datas/new_food-10/\", split=\"validation\")\n",
    "image = ds[\"image\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a61510",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c358e9",
   "metadata": {},
   "source": [
    "使用微調後的模型進行推論最簡單的方法是在 pipline() 中設定。藉由指定的模型建構一個影像分類的 pipeline，然後將影像傳遞給它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model/checkpoint-141/\")\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb61b52",
   "metadata": {},
   "source": [
    "載入影像處理器對影像進行預處理，並以 PyTorch 的張量型態回傳作為輸入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ae3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "import torch\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"my_awesome_food_model/checkpoint-141/\")\n",
    "inputs = image_processor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c9986",
   "metadata": {},
   "source": [
    "將輸入傳遞給模型，並回傳 logits（尚未經過 softmax）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"my_awesome_food_model/checkpoint-141/\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e92350",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = logits.argmax(-1).item()\n",
    "model.config.id2label[predicted_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27d90b",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion - HuggingFace Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa35714",
   "metadata": {},
   "source": [
    "![](https://hackmd.io/_uploads/HkEzgZdwh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60dfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
