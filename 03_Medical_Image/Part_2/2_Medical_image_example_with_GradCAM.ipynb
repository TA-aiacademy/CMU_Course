{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5496c87f",
   "metadata": {},
   "source": [
    "# Image classification for Medical Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef3831",
   "metadata": {},
   "source": [
    "以下將演示如何使用 huggingface 框架並使用其中提供的函數載入自定義的資料集，以達到影像分類的結果。\n",
    "\n",
    "huggingface 的工作流程：\n",
    "![](https://hackmd.io/_uploads/HkEzgZdwh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝所需套件\n",
    "!pip -q install transformers==4.30.0 datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcecd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入相關套件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8e511",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/home/jovyan/ta-shared-ii/datas/new_NIH_data_1800/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fe17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe06ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item['label'])\n",
    "item['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item['image'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74908043",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8868463",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset.features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f85f1",
   "metadata": {},
   "source": [
    "現在，按照標籤 id 轉換成名稱："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40578e",
   "metadata": {},
   "source": [
    "## Preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8302b26",
   "metadata": {},
   "source": [
    "接下來的步驟是載入指定模型使用的影像處理器，將影像處理成張量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# checkpoint = \"google/vit-base-patch16-224-in21k\"  # model name\n",
    "# checkpoint = \"google/efficientnet-b6\"\n",
    "checkpoint = \"microsoft/cvt-13\"\n",
    "# checkpoint = \"microsoft/resnet-50\"\n",
    "# checkpoint = \"facebook/convnext-tiny-224\"\n",
    "# checkpoint = \"facebook/convnext-base-224\"\n",
    "# # checkpoint = \"facebook/convnext-large-224\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a428619",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ab825",
   "metadata": {},
   "source": [
    "---\n",
    "將影像進行轉換，使模型更具一般性以應付過擬合的情況。這裡會使用的 torchvision 中 transforms 的模組，但也能替換成其他適用的影像處理套件。\n",
    "\n",
    "將其調整影像大小以及隨機仿射處理，並使用影像的平均值和標準差進行標準化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f872f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, RandomAffine, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([Resize(size),\n",
    "#                        RandomHorizontalFlip(),\n",
    "                       RandomAffine(degrees=10, scale=(0.9, 1.1)),\n",
    "                       ToTensor(),\n",
    "                       normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bdc20",
   "metadata": {},
   "source": [
    "接下來創建一個預處理函數，轉換並回傳影像的像素值作為模型的輸入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a08ca3",
   "metadata": {},
   "source": [
    "要在整個資料集上應用預處理函數，可以使用 Hugging Face 資料集的 [with_transform](https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.Dataset.with_transform) 方法。當載入資料集的一個元素時，轉換會即時套用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_dataset.with_transform(transforms)\n",
    "valid_ds = valid_dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95597985",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ba826",
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b312907",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item['label'])\n",
    "print(item['pixel_values'].size())\n",
    "plt.imshow(torch.permute(item['pixel_values'], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b6a31",
   "metadata": {},
   "source": [
    "現在使用 [DefaultDataCollator](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/data_collator#transformers.DefaultDataCollator) 創建一個批次樣本。與 Hugging face 裡 Transformers 的其他資料收集器不同，DefaultDataCollator 不會套用額外的預處理，例如填充（padding）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efcc14",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b1f6e",
   "metadata": {},
   "source": [
    "在訓練過程中加入評估指標通常有助於評估模型的表現。可以使用 Hugging Face 的 [Evaluate](https://huggingface.co/docs/evaluate/index) 函式庫快速載入評估方法。在此任務上載入 [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) 指標（請參閱 Hugging Face 的 Evaluate [快速導覽](https://huggingface.co/docs/evaluate/a_quick_tour)，以了解如何載入和計算指標的詳細資訊）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b031e9b",
   "metadata": {},
   "source": [
    "然後創建一個函數，將預測及標籤使用 [compute](https://huggingface.co/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute) 以計算準確度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071960c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e31a0",
   "metadata": {},
   "source": [
    "現在已準備好開始訓練模型了！使用 [AutoModelForImageClassification](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoModelForImageClassification) 載入模型。指定標籤的數量以及標籤的對應方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d347a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True  # 預訓練模型的分類數量與自定義的資料集不同時使用\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa9ddc",
   "metadata": {},
   "source": [
    "接著的階段，只剩以下三個步驟：\n",
    "\n",
    "1. 在 [TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments) 中定義訓練的超參數。請務必留意資料集中未使用的資訊，設定 remove_unused_columns=False 可以防止被刪除未使用到的資訊！例如 image，這會導致無法獲得 pixel_values。另一個必需設定的參數是 output_dir，指定模型儲存的位置。通過設定 push_to_hub=True 將模型上傳至 Hub（需要登入 Hugging Face 才能上傳模型）。在每個 epoch 結束時，[Trainer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer) 將評估準確性並儲存訓練模型。\n",
    "2. 將訓練參數、模型、資料集、預處理器、資料收集器以及計算評估指標函數傳遞給 [Trainer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer)。\n",
    "3. 呼叫 [train](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer.train) 來微調模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layers without training\n",
    "for param in model.cvt.encoder.stages[:1].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f87f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_cvt_model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import MLflowCallback\n",
    "trainer.remove_callback(MLflowCallback)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34ed1e",
   "metadata": {},
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c40214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "valid_acc = []\n",
    "for i in range(0, len(trainer.state.log_history)-1, 2):\n",
    "    train_loss.append(trainer.state.log_history[i]['loss'])\n",
    "    valid_loss.append(trainer.state.log_history[i+1]['eval_loss'])\n",
    "    valid_acc.append(trainer.state.log_history[i+1]['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12138ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(train_loss)), train_loss, label='train')\n",
    "plt.plot(range(len(valid_loss)), valid_loss, label='val')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(valid_acc)), valid_acc)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_metric = evaluate.load(\"BucketHeadP65/confusion_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a870ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99780fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(test_dataset=valid_ds)\n",
    "cm = cfm_metric.compute(predictions=np.argmax(outputs.predictions, axis=1),\n",
    "                        references=outputs.label_ids)['confusion_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.heatmap(cm, annot=True)\n",
    "ax.set(xlabel=\"prediction\", ylabel=\"label\")\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7587c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512656b",
   "metadata": {},
   "source": [
    "現在，微調後的模型以存放在指定路徑，並可使用它來進行推論！\n",
    "\n",
    "載入想要進行推論的影像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test']\n",
    "image = test_dataset['image'][0]\n",
    "label = test_dataset['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ed899",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04443e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56452da",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e4032",
   "metadata": {},
   "source": [
    "使用微調後的模型進行推論最簡單的方法是在 pipline() 中設定。藉由指定的模型建構一個影像分類的 pipeline，然後將影像傳遞給它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"my_cvt_model/checkpoint-380/\")\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39434e2a",
   "metadata": {},
   "source": [
    "載入影像處理器對影像進行預處理，並以 PyTorch 的張量型態回傳作為輸入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "import torch\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"my_cvt_model/checkpoint-380/\")\n",
    "inputs = image_processor(image.convert('RGB'), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c39005",
   "metadata": {},
   "source": [
    "將輸入傳遞給模型，並回傳 logits（尚未經過 softmax）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"my_cvt_model/checkpoint-380/\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837854ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = logits.argmax(-1).item()\n",
    "model.config.id2label[predicted_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf2cb0",
   "metadata": {},
   "source": [
    "* ### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45790e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(test_dataset=test_ds)\n",
    "cm = cfm_metric.compute(predictions=np.argmax(outputs.predictions, axis=1),\n",
    "                        references=outputs.label_ids)['confusion_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc591021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.heatmap(cm, annot=True)\n",
    "ax.set(xlabel=\"prediction\", ylabel=\"label\")\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d30ace",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6d77d",
   "metadata": {},
   "source": [
    "## Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b2f55",
   "metadata": {},
   "source": [
    "可解釋性人工智慧（Explainable AI）是指能夠解釋和解釋其結論、推論和決策過程的人工智慧系統。傳統上，許多機器學習和深度學習模型往往被視為黑盒，它們可以對數據進行複雜的計算，但很難理解為什麼做出特定的預測或決策。\n",
    "\n",
    "其中 GradCAM（Gradient-weighted Class Activation Mapping）用於視覺任務中對深度學習模型的解釋。它可以幫助我們理解模型在圖像分類或物體檢測等任務中的決策過程，並提供對模型預測的視覺化解釋。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f1076",
   "metadata": {},
   "source": [
    "![](https://hackmd.io/_uploads/Syf544uP3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4b519",
   "metadata": {},
   "source": [
    "以下是使用 GradCAM 進行可解釋性 AI 的基本步驟：\n",
    "\n",
    "1. 準備模型：首先，需要準備一個已經訓練好的深度學習模型，例如卷積神經網絡（CNN）用於圖像分類。\n",
    "2. 特徵提取：通過將圖像輸入模型，獲得中間層的特徵圖。這些特徵圖是在網絡中不同層次上提取的高級視覺特徵。\n",
    "3. 梯度計算：對於目標類別，計算反向傳播的梯度。這些梯度表示目標類別相對於特徵圖的重要性。\n",
    "4. 特徵加權：將梯度與特徵圖進行加權相乘，以強調對目標類別貢獻最大的區域。\n",
    "5. 生成顯著性圖：將加權特徵圖進行空間平均或加總，得到一個顯著性圖，表示模型對於不同區域的關注程度。\n",
    "6. 視覺化解釋：將顯著性圖與原始圖像進行融合或覆蓋，生成一張視覺化的解釋圖。這個解釋圖可以顯示出模型在預測中關注的重要區域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde21b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝支援 PyTorch 的 GradCAM 套件\n",
    "!pip -q install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ca3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入相關套件\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from typing import List, Callable, Optional\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbff6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset['test'][\"image\"][0]\n",
    "label = dataset['test']['label'][0]\n",
    "\n",
    "img_tensor = Compose([Resize(size), ToTensor(), normalize])(image.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照不同的模型選定 reshape 的方式\n",
    "def reshape_transform_cvt_huggingface(tensor, model, width, height):\n",
    "    tensor = tensor[:, 1 :, :]  # remove 1 cls token\n",
    "    tensor = tensor.reshape(tensor.size(0),\n",
    "                            height,\n",
    "                            width,\n",
    "                            tensor.size(-1))\n",
    "    \n",
    "    # https://github.com/huggingface/transformers/blob/a2c90a7f7b1f8a2a8217c962a04a1a65638121d5/src/transformers/models/cvt/modeling_cvt.py#L699\n",
    "    norm = model.layernorm(tensor)\n",
    "    return norm.transpose(2, 3).transpose(1, 2)  # (Batch, features, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffff836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_gradcam_transform_cvt_huggingface(tensor, model, width, height):\n",
    "    tensor = tensor[:, 1 :, :]  # remove 1 cls token\n",
    "    tensor = tensor.reshape(tensor.size(0),\n",
    "                            height,\n",
    "                            width,\n",
    "                            tensor.size(-1))\n",
    "    return tensor.transpose(2, 3).transpose(1, 2)  # (Batch, features, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ef65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拿取模型輸出的 logits\n",
    "class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb65169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_cam_on_image(model, #: torch.nn.Module,\n",
    "                          target_layer, #: torch.nn.Module,\n",
    "                          targets_for_gradcam, #: List[Callable],\n",
    "                          reshape_transform, #: Optional[Callable],\n",
    "                          input_tensor=img_tensor, #: torch.nn.Module=img_tensor,\n",
    "                          input_image=image, #: Image=image,\n",
    "                          method=GradCAM): #: Callable=GradCAM):\n",
    "    with method(model=HuggingfaceToTensorModelWrapper(model),\n",
    "                 target_layers=[target_layer],\n",
    "                 reshape_transform=reshape_transform) as cam:\n",
    "\n",
    "        # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n",
    "        repeated_tensor = input_tensor[None, :].repeat(len(targets_for_gradcam), 1, 1, 1)\n",
    "\n",
    "        batch_results = cam(input_tensor=repeated_tensor,\n",
    "                            targets=targets_for_gradcam)\n",
    "        results = []\n",
    "        for grayscale_cam in batch_results:\n",
    "            # convert graycam size to input image size\n",
    "            grayscale_cam = cv2.resize(grayscale_cam, input_image.size) \n",
    "            \n",
    "            visualization = show_cam_on_image(np.float32(input_image)/255,\n",
    "                                              grayscale_cam,\n",
    "                                              use_rgb=True)\n",
    "            # Make it weight less in the notebook:\n",
    "            visualization = cv2.resize(visualization,\n",
    "                                       (visualization.shape[1]//2, visualization.shape[0]//2))\n",
    "            results.append(visualization)\n",
    "        return np.hstack(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced291f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96227adb",
   "metadata": {},
   "source": [
    "選定一張影像作為演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_for_gradcam = [ClassifierOutputTarget(label2id['Atelectasis']),\n",
    "                       ClassifierOutputTarget(label2id['Effusion']),\n",
    "                       ClassifierOutputTarget(label2id['NoFinding'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_transform_gradcam = partial(reshape_gradcam_transform_cvt_huggingface,\n",
    "                            model=model,\n",
    "                            width=img_tensor.shape[2]//16,\n",
    "                            height=img_tensor.shape[1]//16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a741e87",
   "metadata": {},
   "source": [
    "* ### 如何知道應該選擇哪個目標層？**選擇在分類器或池化層之前的最後一層**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd27c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066504ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_gradcam = model.cvt.encoder.stages[-1].layers[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image.fromarray(run_grad_cam_on_image(model=model,\n",
    "                                              target_layer=target_layer_gradcam,\n",
    "                                              targets_for_gradcam=targets_for_gradcam,\n",
    "                                              reshape_transform=reshape_transform_gradcam,\n",
    "                                              input_image=image.convert('RGB'),\n",
    "                                              input_tensor=img_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d6e88",
   "metadata": {},
   "source": [
    "---\n",
    "ScoreCAM（Score-based Class Activation Mapping）和 GradCAM（Gradient-weighted Class Activation Mapping）都是解釋卷積神經網絡（CNN）模型的可視化方法，用於可視化模型對圖像中不同區域的關注程度。它們的主要區別在於**權重的計算方式**和**解釋的焦點**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46febfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import ScoreCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f76134",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image.fromarray(run_grad_cam_on_image(model=model,\n",
    "                                              target_layer=target_layer_gradcam,\n",
    "                                              targets_for_gradcam=targets_for_gradcam,\n",
    "                                              reshape_transform=reshape_transform_gradcam,\n",
    "                                              input_image=image.convert('RGB'),\n",
    "                                              input_tensor=img_tensor,\n",
    "                                              method=ScoreCAM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64454100",
   "metadata": {},
   "source": [
    "* Reference tutorial: https://jacobgil.github.io/pytorch-gradcam-book/HuggingFace.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77a503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
